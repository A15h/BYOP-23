# -*- coding: utf-8 -*-
"""bertmodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sS2viacXRbjrU8X3qANQIiHi0KXaR4sD
"""

!pip install transformers numpy pandas
!pip install datasets
import pandas as pd
! pip install -q kaggle

# Commented out IPython magic to ensure Python compatibility.

import warnings
warnings.filterwarnings('ignore')
# %config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook  | hit tab


from google.colab import files
files.upload()

#!mkdir ~/.kaggle
#!cp kaggle.json ~/.kaggle/
import os
os.environ['KAGGLE_USERNAME'] = 'aishwaryaahuja'
os.environ['KAGGLE_KEY'] = 'f7cb3c6486bbef40a8748c4ac5eb6531'

!kaggle datasets download -d sbhatti/financial-sentiment-analysis -p /content

df = pd.read_csv('/content/financial-sentiment-analysis.zip')
df.head()

df['Sentiment'] = df.Sentiment.astype('category')

!pip install text_hammer
import text_hammer as th
from tqdm import *

def text_preprocessing(df,col_name):
  column = col_name
  df[column] = df[column].progress_apply(lambda x:str(x).lower())
  df[column] = df[column].progress_apply(lambda x:th.cont_exp(x))
  df[column] = df[column].progress_apply(lambda x:th.remove_emails(x))
  df[column] = df[column].progress_apply(lambda x:th.remove_html_tags(x))
  df[column] = df[column].progress_apply(lambda x:th.remove_special_chars(x))
  df[column] = df[column].progress_apply(lambda x:th.remove_accented_chars(x))

  return (df)



df = text_preprocessing(df,'Sentence')

encoding={'neutral':1, 'negative':0, 'positive':2}
df['Sentiment']=df['Sentiment'].map(encoding)

df['words'] = df.Sentence.apply(lambda x:len(x.split()))
df= df[df['Sentiment'].isnull() == False]
df.head()
#df.shape

df.words.max()

df.Sentiment.unique()

from sklearn.model_selection import train_test_split
train,test = train_test_split(df, test_size = 0.3, random_state = 42, stratify = df.Sentiment)
train.shape

!pip install tensorflow keras

from tensorflow.keras.utils import to_categorical
train.head()

from transformers import AutoTokenizer,TFBertModel
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
bert = TFBertModel.from_pretrained('bert-base-cased')

tokenizer.save_pretrained('bert-tokenizer')
bert.save_pretrained('bert-model')
# for saving model locally and we can load it later o

import shutil
shutil.make_archive('bert-tokenizer', 'zip', 'bert-tokenizer')

tokenizer('i am using colab')

x_train = tokenizer(
    text=train.Sentence.tolist(),
    add_special_tokens=True,
    max_length=60,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = True,
    return_attention_mask = True,
    verbose = True)


x_test = tokenizer(
    text=test.Sentence.tolist(),
    add_special_tokens=True,
    max_length=60,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = True,
    return_attention_mask = True,
    verbose = True)

x_train

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.initializers import TruncatedNormal
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.utils import to_categorical

import tensorflow as tf
tf.config.experimental.list_physical_devices('GPU')

max_len = 60
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense

input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
input_mask = Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")
# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]


embeddings = bert(input_ids,attention_mask = input_mask)[0] #(0 is the last hidden states,1 means pooler_output)
out = tf.keras.layers.GlobalMaxPool1D()(embeddings)
out = Dense(128, activation='relu')(out)
out = tf.keras.layers.Dropout(0.1)(out)
out = Dense(32,activation = 'relu')(out)
out = Dense(16,activation = 'relu')(out)
y = Dense(3,activation = 'softmax')(out)

    
model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)
model.layers[2].trainable = True

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import PolynomialDecay

# Define your model architecture here

initial_learning_rate = 5e-5
decay_steps = 5 * len(x_train) // 36 # decay the learning rate every 5 epochs
end_learning_rate = 1e-7
power = 1.0

lr_schedule = PolynomialDecay(
    initial_learning_rate,
    decay_steps,
    end_learning_rate,
    power
)

optimizer = Adam(
    learning_rate=lr_schedule,
    epsilon=1e-08,
    clipnorm=1.0)

loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metric = tf.keras.metrics.BinaryAccuracy(name='accuracy')

model.compile(
    optimizer=optimizer,
    loss=loss, 
    metrics=metric)

train_history = model.fit(
    x={'input_ids': x_train['input_ids'], 'attention_mask': x_train['attention_mask']},
    y=to_categorical(train.Sentiment),
    validation_data=(
        {'input_ids': x_test['input_ids'], 'attention_mask': x_test['attention_mask']}, 
        to_categorical(test.Sentiment)
    ),
    epochs=3,
    batch_size=36
)

model.save_weights('sentiment_weights.h5')
metrics_df = pd.DataFrame(train_history.history)
print(metrics_df)

import numpy as np
texts = input(str('Enter the text: '))
x_val = tokenizer(text=texts,add_special_tokens=True,max_length=60,truncation=True,padding='max_length',return_tensors='tf',return_token_type_ids=True,return_attention_mask=True,verbose=True)
validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100
pred_val = np.argmax(validation,axis = 1)
if pred_val==0:
  print("Negative")
elif pred_val==1:
  print("Neutral")
else:
  print("Positive")







